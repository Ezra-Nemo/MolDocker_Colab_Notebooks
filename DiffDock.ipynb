{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kKVmji4OvvFE"
   },
   "outputs": [],
   "source": [
    "#@title Install Dependencies\n",
    "%%capture\n",
    "!pip install -q e3nn==0.5.1 fair-esm==2.0.0 networkx==3.2.1 pybind11==2.12.0 rdkit==2023.9.6 requests==2.32.3 scikit-learn==1.5.0 torch-geometric==2.2.0 prody pydantic==2.10.6 gradio==4.44.1\n",
    "!pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "No0sJCpuvvFE"
   },
   "outputs": [],
   "source": [
    "#@title Clone DiffDock\n",
    "%%capture\n",
    "!git clone https://github.com/gcorso/DiffDock.git\n",
    "%cd DiffDock\n",
    "!gdown --fuzzy \"https://drive.google.com/file/d/1CFuI1XEJpEm-jTGRRmD7FjGhJhMIE3vJ/view?usp=sharing\"\n",
    "!unzip diffdock_cache.zip\n",
    "\n",
    "import esm\n",
    "_ = esm.pretrained.esm2_t33_650M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yL9TwnpovvFI"
   },
   "outputs": [],
   "source": [
    "#@title Gradio Interface\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import pickle\n",
    "import zipfile\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "atom_term_compiled = re.compile(r'(ATOM|TER).*')\n",
    "aa_chain_pos_compiled = re.compile(r'([A-Z]{3}).([a-zA-Z0-9])\\s*(-?\\d+)')\n",
    "confidence_compiled = re.compile(r'_confidence(-?\\d+\\.\\d+)\\.sdf')\n",
    "retreive_mdl_compiled = re.compile(r'MODEL\\s+[0-9]+\\s+((\\n|.)*?)ENDMDL')\n",
    "retrieve_line_without_na_compiled = re.compile(r'\\n(ATOM|TER.).{13}(?!\\s+(DT|DA|DC|DG|DI|A|U|C|G|I)).*')\n",
    "\n",
    "atom_type_map = {'HD': 'H', 'HS': 'H',\n",
    "                 'NA': 'N', 'NS': 'N',\n",
    "                 'A' : 'C', 'G' : 'C', 'CG0': 'C', 'CG1': 'C', 'CG2': 'C', 'CG3': 'C', 'G0': 'C', 'G1': 'C', 'G2': 'C', 'G3': 'C',\n",
    "                 'OA': 'O', 'OS': 'O',\n",
    "                 'SA': 'S'}\n",
    "\n",
    "def pdbqt_to_pdb(pdbqt_str: str):\n",
    "    meet_end_of_chain = 0\n",
    "    sub_re_map = {'HIS': re.compile(r'HID|HIP|HIE'),\n",
    "                  'GLU': re.compile(r'GLH'),\n",
    "                  'ASP': re.compile(r'ASH'),\n",
    "                  'LYS': re.compile(r'LYN'),\n",
    "                  'CYS': re.compile(r'CYM|CYX'),}\n",
    "    last_chain, last_atomidx, last_resname, last_respos = None, None, None, None\n",
    "    def map_pdbqt_line_to_pdb(line: str, idx: int):\n",
    "        nonlocal meet_end_of_chain, last_chain, last_resname, last_respos, last_atomidx\n",
    "        atom_type = line[77:].strip()\n",
    "        if atom_type in atom_type_map:\n",
    "            atom_type = atom_type_map[atom_type]\n",
    "        chain = line[21]\n",
    "        res_name = line[17:20].strip()\n",
    "        res_pos = int(line[22:26].strip())\n",
    "        final = []\n",
    "        if chain != last_chain and last_chain is not None:\n",
    "            ter = {'atom_idx': last_atomidx + 1,\n",
    "                   'res_name': last_resname    ,\n",
    "                   'chain'   : last_chain      ,\n",
    "                   'res_pos' : last_respos     ,\n",
    "                   }\n",
    "            final.append(ter)\n",
    "            meet_end_of_chain += 1\n",
    "        atom_idx = idx + meet_end_of_chain\n",
    "        line_dict = {'atom_idx'   : atom_idx                  ,\n",
    "                     'atom_name'  : line[12:16].strip()       ,\n",
    "                     'alt_id'     : line[16]                  ,\n",
    "                     'res_name'   : res_name                  ,\n",
    "                     'chain'      : chain                     ,\n",
    "                     'res_pos'    : res_pos                   ,\n",
    "                     'others'     : line[26:66]               ,  # skip partial charge\n",
    "                     'atom_type'  : atom_type                 ,  # don't strip to keep spaces\n",
    "                     }\n",
    "        final.append(line_dict)\n",
    "        last_chain, last_resname, last_respos, last_atomidx = chain, res_name, res_pos, atom_idx\n",
    "        return final\n",
    "    \n",
    "    def convert_to_pdb_str(atom_data: list):\n",
    "        lines = []\n",
    "        format_str = \"ATOM  {:5d} {:4s}{:1s}{:3s} {:1s}{:4d}{:40s}            {}\\n\"\n",
    "        term_str   = \"TER   {:5d}      {:3s} {:1s}{:4d}\\n\"\n",
    "        for entry in atom_data:\n",
    "            if len(entry) == 8:\n",
    "                lines.append(format_str.format(\n",
    "                    entry['atom_idx'], entry['atom_name'], entry['alt_id'], \n",
    "                    entry['res_name'], entry['chain'], entry['res_pos'],\n",
    "                    entry['others'], entry['atom_type'],\n",
    "                ))\n",
    "            else:\n",
    "                lines.append(term_str.format(\n",
    "                    entry['atom_idx'], entry['res_name'], \n",
    "                    entry['chain'], entry['res_pos'],\n",
    "                ))\n",
    "        return ''.join(lines)\n",
    "    \n",
    "    for aa, re_comp in sub_re_map.items():\n",
    "        pdbqt_str = re.sub(re_comp, aa, pdbqt_str)\n",
    "    \n",
    "    protein_data = [item for idx, line in enumerate(pdbqt_str.strip().splitlines()) if line.startswith('ATOM')\n",
    "                    for item in map_pdbqt_line_to_pdb(line, idx)]\n",
    "    final_ter = {'atom_idx': last_atomidx + 1,\n",
    "                 'res_name': last_resname    ,\n",
    "                 'chain'   : last_chain      ,\n",
    "                 'res_pos' : last_respos     ,\n",
    "                 }\n",
    "    protein_data.append(final_ter)\n",
    "    \n",
    "    return protein_data, convert_to_pdb_str(protein_data)\n",
    "\n",
    "class PDBEditor:\n",
    "    def __init__(self, pdbqt_str: str | None=None):\n",
    "        self.pdbqt_str = pdbqt_str\n",
    "        self.pdb_chain_dict = {}\n",
    "    \n",
    "    def parse_pdb_text_to_dict(self, display_flex_dict = None):\n",
    "        self.pdb_chain_dict = {}\n",
    "        chain_aa_dict = {}\n",
    "        for atom_term_line in re.finditer(atom_term_compiled, self.pdbqt_str):\n",
    "            line = atom_term_line.group(0)\n",
    "            aa, chain, aa_pos = re.search(aa_chain_pos_compiled, line).group(1,2,3)\n",
    "            aa_pos = int(aa_pos)\n",
    "            if chain not in self.pdb_chain_dict:\n",
    "                self.pdb_chain_dict[chain] = {}\n",
    "                chain_aa_dict[chain] = []\n",
    "            aa_pos_dict = self.pdb_chain_dict[chain]\n",
    "            if aa_pos not in aa_pos_dict:\n",
    "                aa_pos_dict[aa_pos] = []\n",
    "                chain_aa_dict[chain].append(aa)\n",
    "            aa_pos_dict[aa_pos].append(line)\n",
    "        for chain, aa_pos_dict in self.pdb_chain_dict.items():\n",
    "            aa_cnt = len(aa_pos_dict)\n",
    "            for pos, text_list in aa_pos_dict.items():\n",
    "                aa_pos_dict[pos] = '\\n'.join(text_list)\n",
    "            self.pdb_chain_dict[chain] = pd.DataFrame.from_dict(aa_pos_dict, 'index')\n",
    "            if display_flex_dict is None:\n",
    "                self.pdb_chain_dict[chain]['Display'] = [True] * aa_cnt\n",
    "                self.pdb_chain_dict[chain]['Flexible'] = [False] * aa_cnt\n",
    "            else:\n",
    "                self.pdb_chain_dict[chain]['Display'] = display_flex_dict[chain]['Display']\n",
    "                self.pdb_chain_dict[chain]['Flexible'] = display_flex_dict[chain]['Flexible']\n",
    "            self.pdb_chain_dict[chain]['AA_Name'] = chain_aa_dict[chain]\n",
    "        \n",
    "    def parse_logic(self, series: pd.Series, logic: str):\n",
    "        def replace_expression(match):\n",
    "            expr = match.group()\n",
    "            negation = ''\n",
    "            if expr.startswith('~'):\n",
    "                negation = '~'\n",
    "                expr = expr[1:]\n",
    "            if '-' in expr:  # range\n",
    "                start, end = map(int, expr.rsplit('-', 1))\n",
    "                return f\"{negation}((series.index >= {start}) & (series.index <= {end}))\"\n",
    "            else:  # single value\n",
    "                return f\"{negation}(series.index == {expr})\"\n",
    "        \n",
    "        logic = logic.replace(' ', '').replace(',', '|')    # \",\" is the same as or \"|\"\n",
    "        logic_eval = re.sub(r'~?-?\\d+-\\d+|~?\\d+(?:,~?\\d+)*', replace_expression, logic)\n",
    "        \n",
    "        try:\n",
    "            result = pd.eval(logic_eval, local_dict={'series': series}, engine='python') # need to pass local_dict or else Nuitka compiled code won't work\n",
    "            series[result] = True\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        return series\n",
    "    \n",
    "    def update_display(self, chain: str, display_str: str | None):\n",
    "        full_df = self.pdb_chain_dict[chain]\n",
    "        if display_str is None:\n",
    "            display_series = pd.Series([False] * len(full_df), list(full_df.index))\n",
    "            full_df['Display'] = display_series\n",
    "            return\n",
    "        elif not display_str:\n",
    "            display_series = pd.Series([True] * len(full_df), list(full_df.index))\n",
    "            full_df['Display'] = display_series\n",
    "            return\n",
    "        display_series = pd.Series([False] * len(full_df), list(full_df.index)) # default to False\n",
    "        result = self.parse_logic(display_series, display_str)\n",
    "        if result is None:\n",
    "            return f'Invalid syntax for chain {chain}.'\n",
    "        full_df['Display'] = display_series\n",
    "    \n",
    "    def _condense_to_range(self, list_of_nums: list[int]):\n",
    "        final_text = ''\n",
    "        start = list_of_nums[0]\n",
    "        end = list_of_nums[0]\n",
    "        range_cnt = 1\n",
    "        for num in list_of_nums[1:]:\n",
    "            if num == start + range_cnt:\n",
    "                end = num\n",
    "                range_cnt += 1\n",
    "            else:\n",
    "                if start == end:\n",
    "                    final_text += f'{start}|'\n",
    "                else:\n",
    "                    final_text += f'{start}-{end},'\n",
    "                start = num\n",
    "                end = num\n",
    "                range_cnt = 1\n",
    "        if start == end:\n",
    "            final_text += f'{start}'\n",
    "        else:\n",
    "            final_text += f'{start}-{end}'\n",
    "        return final_text\n",
    "    \n",
    "    def convert_to_range_text(self, chain: str):\n",
    "        s = self.pdb_chain_dict[chain]['Display']\n",
    "        mask = s == True\n",
    "        displayed_pos = s[mask].index.to_list()\n",
    "        if not displayed_pos:\n",
    "            return f'~1-{max(s.index.to_list())}'\n",
    "        if len(displayed_pos) == len(s):\n",
    "            return ''   # empty text if everything is displayed\n",
    "        return self._condense_to_range(displayed_pos)\n",
    "    \n",
    "    def convert_full_dict_to_text(self):\n",
    "        protein_strs = []\n",
    "        for df in self.pdb_chain_dict.values():\n",
    "            string = '\\n'.join(df[0].to_list())\n",
    "            protein_strs.append(string)\n",
    "        return '\\n'.join(protein_strs)\n",
    "    \n",
    "    def convert_dict_to_pdb_text(self, return_scheme=False):\n",
    "        pdbqt_str = ''\n",
    "        cnt = 0\n",
    "        for df in self.pdb_chain_dict.values():\n",
    "            mask = df['Display'] == True\n",
    "            string = '\\n'.join(df[0][mask].to_list())\n",
    "            if string:\n",
    "                cnt += 1\n",
    "            pdbqt_str += string\n",
    "            if string:\n",
    "                pdbqt_str += '\\n'\n",
    "        if return_scheme:\n",
    "            if cnt > 1:\n",
    "                scheme = 'chainindex'\n",
    "            else:\n",
    "                scheme = 'residueindex'\n",
    "            return pdbqt_str, scheme\n",
    "        return pdbqt_str\n",
    "    \n",
    "    def check_format_type(self):\n",
    "        for chain_dict in self.pdb_chain_dict.values():\n",
    "            line = chain_dict.iloc[0, 0]\n",
    "            if line[70:76].strip():\n",
    "                return 'pdbqt'\n",
    "            else:\n",
    "                return 'pdb'\n",
    "\n",
    "def convert_row_to_string_float(csv_row_str: str):\n",
    "    name, score = csv_row_str.strip().split(',')\n",
    "    score = float(score)\n",
    "    return name, score\n",
    "\n",
    "class DiffDockInterface:\n",
    "    def setup_interface(self):\n",
    "        self.pdb_editor = None\n",
    "        self.process = None\n",
    "        self.curr_docking = False\n",
    "        self.curr_dir = os.path.abspath('')\n",
    "        self.dock_input_dir = os.path.join(self.curr_dir, 'dock_input')\n",
    "        self.dock_output_dir = os.path.join(self.curr_dir, 'dock_output')\n",
    "        self.zipped_dir = os.path.join(self.curr_dir, 'zipped_files')\n",
    "        self.cache_dir = os.path.join(self.dock_output_dir, 'cache_files')\n",
    "        os.makedirs(self.dock_input_dir, exist_ok=True)\n",
    "        os.makedirs(self.dock_output_dir, exist_ok=True)\n",
    "        os.makedirs(self.zipped_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        self.log_file = os.path.join(self.cache_dir, 'docking_log.log')\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            ...\n",
    "        self.csv_file = os.path.join(self.cache_dir, 'confidence_log.csv')\n",
    "        with open(self.csv_file, 'w') as f:\n",
    "            f.write('Name,Confidence\\n')\n",
    "        self.recorded_names = []\n",
    "        with gr.Blocks(css='footer{display:none !important}') as Interface:\n",
    "            gr.Markdown('<span style=\"font-size:25px; font-weight:bold; \">DiffDock Interface</span>')\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown('<span style=\"font-size:20px; font-weight:bold; \">Protein PDB/CIF</span>')\n",
    "                    protein_stat = gr.Textbox(label='Protein Status :')\n",
    "                    protein_input = gr.File(file_count='single',\n",
    "                                            file_types=['.pdb', '.mds'],\n",
    "                                            label='Upload Protein PDB/CIF or MDS file')\n",
    "                with gr.Column():\n",
    "                    gr.Markdown('<span style=\"font-size:20px; font-weight:bold; \">DiffDock CSV</span>')\n",
    "                    ligand_stat = gr.Textbox(label='Ligand Status :')\n",
    "                    ligand_input = gr.File(file_count='single',\n",
    "                                            file_types=['.csv'],\n",
    "                                            label='Upload CSV File (prepared with Molecule Converter)')\n",
    "            with gr.Row():\n",
    "                display_text = gr.Textbox(label='Display',\n",
    "                                          placeholder='A:1-15,20-35 B:15-20',\n",
    "                                          interactive=True)\n",
    "            with gr.Row():\n",
    "                dock_progress = gr.Text(label='Progress',\n",
    "                                        interactive=False,\n",
    "                                        scale=4,\n",
    "                                        max_lines=15,)\n",
    "                with gr.Column(scale=1):\n",
    "                    dock_button = gr.Button('Dock', interactive=False)\n",
    "                    zip_checkbox = gr.Checkbox(label='Zip Result')\n",
    "                    zip_progress = gr.Textbox(label='Progress', interactive=False)\n",
    "                    zip_name = gr.Textbox(label='Zip File Name',\n",
    "                                          placeholder='docked_result',\n",
    "                                          interactive=True)\n",
    "                    zipped_file_output = gr.File(label='Zipped file',\n",
    "                                                 file_count='single',\n",
    "                                                 file_types=['.zip'])\n",
    "            dock_status = gr.Textbox(label='Docking Status :')\n",
    "            energy_df = gr.DataFrame(headers=['Name', 'Confidence'],\n",
    "                                     type='array',\n",
    "                                     interactive=False)\n",
    "            self.protein_loaded = False\n",
    "            self.ligand_laoded = False\n",
    "            protein_input.change(self.upload_protein_file, inputs=protein_input, outputs=[protein_stat, dock_button, display_text])\n",
    "            ligand_input.change(self.upload_ligand_file, inputs=ligand_input, outputs=[ligand_stat, dock_button])\n",
    "            dock_button.click(self.start_docking, inputs=display_text, outputs=None)\n",
    "            \n",
    "            Interface.load(self.read_docked_progress, None, dock_progress, every=1)\n",
    "            Interface.load(self.check_curr_docking_status, None, [dock_status, dock_button], every=3)\n",
    "            Interface.load(self.new_dataframe, None, energy_df, every=1)\n",
    "            Interface.load(self.check_confidence, None, None, every=1)\n",
    "            Interface.load(self.zip_docked_files, [zip_checkbox, zip_name],\n",
    "                           [zipped_file_output, zip_checkbox, zip_progress], every=1)\n",
    "            Interface.queue().launch(share=True)\n",
    "            \n",
    "    def upload_protein_file(self, file):\n",
    "        display_update = gr.update()\n",
    "        if file:\n",
    "            if file.endswith('.pdb'):\n",
    "                with open(file) as f:\n",
    "                    pdb_str = f.read()\n",
    "                self.pdb_editor = PDBEditor(pdb_str)\n",
    "                self.pdb_editor.parse_pdb_text_to_dict()\n",
    "                self.protein_loaded = True\n",
    "                return 'Protein file loaded (pdb)', gr.update(interactive=self.protein_loaded & self.ligand_laoded), display_update\n",
    "            else:\n",
    "                with open(file, 'rb') as f:\n",
    "                    mds_dict = pickle.load(f)\n",
    "                if 'pdbqt_editor' in mds_dict:\n",
    "                    setting_dict = mds_dict['pdbqt_editor']\n",
    "                    self.pdb_editor = PDBEditor()\n",
    "                    self.pdb_editor.pdb_chain_dict = setting_dict\n",
    "                    if self.pdb_editor.check_format_type() == 'pdbqt':\n",
    "                        full_str = self.pdb_editor.convert_full_dict_to_text()\n",
    "                        display_flex_dict = {}\n",
    "                        _, string = pdbqt_to_pdb(full_str)\n",
    "                        for chain, df in self.pdb_editor.pdb_chain_dict.items():\n",
    "                            display, flex = df['Display'], df['Flexible']\n",
    "                            display_flex_dict[chain] = {'Display': display.to_list(), 'Flexible': flex.to_list()}\n",
    "                        self.pdb_editor = PDBEditor(string)\n",
    "                        self.pdb_editor.parse_pdb_text_to_dict(display_flex_dict)\n",
    "                    display_strs = []\n",
    "                    for chain in setting_dict:\n",
    "                        r = self.pdb_editor.convert_to_range_text(chain)\n",
    "                        if r:\n",
    "                            display_strs.append(f'{chain}:{r}')\n",
    "                    display_update = gr.update(value=' '.join(display_strs))\n",
    "                    self.protein_loaded = True\n",
    "                    return 'Protein file loaded (mds)', gr.update(interactive=self.protein_loaded & self.ligand_laoded), display_update\n",
    "                else:\n",
    "                    self.pdb_editor = None\n",
    "                    self.protein_loaded = False\n",
    "                    return 'Protein not found in mds file', gr.update(interactive=False), display_update\n",
    "        self.pdb_editor = None\n",
    "        self.protein_loaded = False\n",
    "        return 'Protein file removed', gr.update(interactive=False), display_update\n",
    "    \n",
    "    def upload_ligand_file(self, file):\n",
    "        csv_pth = os.path.join(self.dock_input_dir, 'ligand.csv')\n",
    "        pdb_pth = os.path.join(self.cache_dir, 'protein_processed.pdb')\n",
    "        if file:\n",
    "            new_csv_str = []\n",
    "            with open(file) as f:\n",
    "                new_csv_str = [l+',protein_path,protein_sequence' if i == 0 \n",
    "                               else l+f',{pdb_pth},' \n",
    "                               for i, l in enumerate(f.read().strip().splitlines())]\n",
    "            self.total_ligands = len(new_csv_str) - 1\n",
    "            new_csv_str = '\\n'.join(new_csv_str)\n",
    "            with open(csv_pth, 'w') as f:\n",
    "                f.write(new_csv_str)\n",
    "            self.ligand_laoded = True\n",
    "            return 'Ligand file loaded', gr.update(interactive=self.protein_loaded & self.ligand_laoded)\n",
    "        os.remove(csv_pth)\n",
    "        self.ligand_laoded = False\n",
    "        return 'Ligand file removed', gr.update(interactive=False)\n",
    "    \n",
    "    def check_curr_docking_status(self):\n",
    "        if self.process is None:\n",
    "            return 'Not docking', gr.update(interactive=self.protein_loaded & self.ligand_laoded)\n",
    "        else:\n",
    "            if self.curr_docking:\n",
    "                c = 0\n",
    "                for subdir in os.listdir(self.dock_output_dir):\n",
    "                    if subdir != 'cache_files' and subdir.endswith('.sdf'):\n",
    "                        c += 1\n",
    "                if c == self.total_ligands:\n",
    "                    self.curr_docking = False\n",
    "                    return 'Docking done', gr.update(interactive=self.protein_loaded & self.ligand_laoded)\n",
    "                return f'Docking... ( {c} / {self.total_ligands} )', gr.update(interactive=False)\n",
    "            else:\n",
    "                return 'Docking done', gr.update(interactive=self.protein_loaded & self.ligand_laoded)\n",
    "    \n",
    "    def check_confidence(self):\n",
    "        if self.process is not None:\n",
    "            retrieved = []\n",
    "            for mol_name in os.listdir(self.dock_output_dir):\n",
    "                if mol_name != 'cache_files' and mol_name not in self.recorded_names and not mol_name.endswith('.sdf'):\n",
    "                    subdir = os.path.join(self.dock_output_dir, mol_name)\n",
    "                    subdir_files = os.listdir(subdir)\n",
    "                    if subdir_files:\n",
    "                        mols_aff_map = {}\n",
    "                        for f in subdir_files:\n",
    "                            if f.startswith('rank1_'):\n",
    "                                conf = re.search(confidence_compiled, f).group(1)\n",
    "                                retrieved.append([mol_name, conf])\n",
    "                                self.recorded_names.append(mol_name)\n",
    "                            if '_confidence' in f:\n",
    "                                with open(os.path.join(subdir, f)) as sdf_f:\n",
    "                                    mol_block = sdf_f.read()\n",
    "                                aff = f.rsplit('_confidence', 1)[-1].split('.sdf')[0]\n",
    "                                mols_aff_map[Chem.MolFromMolBlock(mol_block)] = float(aff)\n",
    "                        mols_aff_map = dict(sorted(mols_aff_map.items(), reverse=True, key=lambda x: x[1]))\n",
    "                        target_sdf_file = os.path.join(self.dock_output_dir, mol_name+'_out.sdf')\n",
    "                        with Chem.SDWriter(target_sdf_file) as writer:\n",
    "                            for mol, aff in mols_aff_map.items():\n",
    "                                mol.SetProp('Score', f'ENERGY= {aff}  LOWER_BOUND= 0.000  UPPER_BOUND= 0.000')\n",
    "                                writer.write(mol)\n",
    "                        shutil.rmtree(subdir)\n",
    "            if retrieved:\n",
    "                with open(self.csv_file, 'a') as f:\n",
    "                    for name_conf in retrieved:\n",
    "                        f.write(f'{name_conf[0]},{name_conf[1]}\\n')\n",
    "    \n",
    "    def _parse_display_string(self, display_res: str):\n",
    "        display_res_list = display_res.split()\n",
    "        if not display_res_list:\n",
    "            return\n",
    "        for display in display_res_list:\n",
    "            chain, res = display.split(':')\n",
    "            self.pdb_editor.update_display(chain, res)\n",
    "    \n",
    "    def start_docking(self, display_text):\n",
    "        csv = os.path.join(self.dock_input_dir, 'ligand.csv')\n",
    "        pdb_pth = os.path.join(self.cache_dir, 'protein_processed.pdb')\n",
    "        self._parse_display_string(display_text)\n",
    "        pdb_text = self.pdb_editor.convert_dict_to_pdb_text()\n",
    "        with open(pdb_pth, 'w') as f:\n",
    "            f.write(pdb_text)\n",
    "        self.curr_docking = True\n",
    "        with open(self.log_file, 'w') as log_f:\n",
    "            self.process = subprocess.Popen(['python', 'inference.py',\n",
    "                                             '--config', 'default_inference_args.yaml',\n",
    "                                             '--protein_ligand_csv', f'{csv}',\n",
    "                                             '--out_dir', f'{self.dock_output_dir}'],\n",
    "                                            stdout=log_f, stderr=log_f)\n",
    "            \n",
    "    def read_docked_progress(self):\n",
    "        with open(self.log_file) as f:\n",
    "          s = f.read()\n",
    "        return s\n",
    "        \n",
    "    def zip_docked_files(self, check_status, zipped_name):\n",
    "        if check_status:\n",
    "            if not zipped_name:\n",
    "                zipped_name = 'docked_result'\n",
    "            zipped_file = os.path.join(self.zipped_dir, zipped_name + '.zip')\n",
    "            all_files = []\n",
    "            for root, dirs, files in os.walk(self.dock_output_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    all_files.append(file_path)\n",
    "            file_cnt = len(all_files)\n",
    "            yield gr.update(), False, f'Progress ({0:{len(str(file_cnt))}}/{file_cnt})'\n",
    "            num = 0\n",
    "            with zipfile.ZipFile(zipped_file, 'w', zipfile.ZIP_LZMA) as zipf:\n",
    "                for file in all_files:\n",
    "                    zipf.write(file, os.path.relpath(file, self.dock_output_dir))\n",
    "                    num += 1\n",
    "                    yield gr.update(), gr.update(), f'Progress ({num:{len(str(file_cnt))}}/{file_cnt})'\n",
    "            yield zipped_file, gr.update(), 'Zipping Done'\n",
    "        else:\n",
    "            yield gr.update(), gr.update(), gr.update()\n",
    "        \n",
    "    def new_dataframe(self):\n",
    "        with open(self.csv_file) as f:\n",
    "            text = f.readlines()[1:]\n",
    "        array = [convert_row_to_string_float(l) for l in text if l.strip()]\n",
    "        if array:\n",
    "            return gr.update(value=array)\n",
    "        else:\n",
    "            return gr.update(value=None)\n",
    "\n",
    "\n",
    "interface = DiffDockInterface()\n",
    "interface.setup_interface()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vina_gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
